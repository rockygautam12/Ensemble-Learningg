{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb92eba-006a-4945-8b67-b93a4dce99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
    "behind it.\n",
    "\n",
    "Ensemble Learning in machine learning is a technique where multiple models (often called \"weak learners\") are combined to produce a single, stronger predictive model.\n",
    "\n",
    "Key Idea\n",
    "The main idea is that a group of diverse models, when combined, can outperform any individual model. This is based on the principle that different models make different errors, and by aggregating their predictions, we can reduce the overall error and improve generalization.\n",
    "\n",
    "Why It Works\n",
    "Error reduction – Combining models helps average out mistakes made by individual models.\n",
    "\n",
    "Variance reduction – It smooths out overfitting that may occur with a single model.\n",
    "\n",
    "Bias reduction – Multiple models can capture different patterns in data.\n",
    "\n",
    "Common Types of Ensemble Methods\n",
    "Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Trains multiple models on random subsets of the training data (with replacement).\n",
    "\n",
    "Example: Random Forest.\n",
    "\n",
    "Boosting\n",
    "\n",
    "Trains models sequentially, where each new model focuses on correcting the errors of the previous ones.\n",
    "\n",
    "Example: AdaBoost, Gradient Boosting, XGBoost.\n",
    "\n",
    "Stacking (Stacked Generalization)\n",
    "\n",
    "Combines predictions from several models using another model (meta-learner) to make the final prediction.\n",
    "\n",
    " In short: Ensemble learning is like asking multiple experts for advice instead of relying on just one — the combined wisdom usually gives a more reliable answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a5552-28cf-401a-a0b7-82477eb50974",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: What is the difference between Bagging and Boosting?\n",
    "\n",
    "1. Concept\n",
    "Bagging (Bootstrap Aggregating):\n",
    "Trains multiple models independently on different random subsets of the data, then averages (regression) or votes (classification) on their predictions.\n",
    "Goal: Reduce variance.\n",
    "\n",
    "Boosting:\n",
    "Trains models sequentially, where each new model focuses more on the errors made by the previous models.\n",
    "Goal: Reduce bias and variance.\n",
    "\n",
    "| Aspect             | Bagging                            | Boosting                                       |\n",
    "| ------------------ | ---------------------------------- | ---------------------------------------------- |\n",
    "| **Model Training** | In parallel (independent models)   | Sequential (dependent models)                  |\n",
    "| **Data Sampling**  | Random sampling with replacement   | All data used, but weighted to focus on errors |\n",
    "| **Error Handling** | Errors not given special attention | Each model corrects errors of the previous one |\n",
    "\n",
    "\n",
    "    3. Example Algorithms\n",
    "Bagging: Random Forest, Bagged Decision Trees.\n",
    "\n",
    "Boosting: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "    | Aspect       | Bagging                                    | Boosting                                |\n",
    "| ------------ | ------------------------------------------ | --------------------------------------- |\n",
    "| **Bias**     | Usually similar to the base learner’s bias | Lower bias (focuses on errors)          |\n",
    "| **Variance** | Significantly reduced                      | Reduced, but can overfit if overtrained |\n",
    "\n",
    "\n",
    "\n",
    "    5. Overfitting Risk\n",
    "Bagging: Less prone to overfitting (due to randomness).\n",
    "\n",
    "Boosting: More prone to overfitting if too many iterations are used.\n",
    "\n",
    "Analogy:\n",
    "\n",
    "Bagging → Like asking many people the same question independently, then taking the majority vote.\n",
    "\n",
    "Boosting → Like asking one person, then telling the next person what mistakes were made, so they can improve the answer.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e09d87-33a8-442e-8004-2f73ba38bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
    "like Random Forest?\n",
    "\n",
    "Bootstrap sampling is a statistical technique where we create multiple new datasets by randomly sampling from the original dataset with replacement.\n",
    "Because of the \"with replacement\" rule, the same data point can appear multiple times in one sample, while some points may be missing.\n",
    "\n",
    "Role in Bagging (e.g., Random Forest)\n",
    "In Bagging methods like Random Forest, bootstrap sampling is used to create different training subsets for each model in the ensemble.\n",
    "\n",
    "Why it’s important:\n",
    "Model Diversity –\n",
    "Since each model (tree) is trained on a different random subset, they learn slightly different patterns. This diversity reduces correlation between models.\n",
    "\n",
    "Variance Reduction –\n",
    "By averaging predictions from multiple diverse models, Bagging reduces variance, making the final prediction more stable.\n",
    "\n",
    "Out-of-Bag (OOB) Error Estimation –\n",
    "In bootstrap sampling, about 36.8% of the data is not included in a given sample (on average).\n",
    "These unused points can be used as a built-in validation set to estimate model performance without separate data splitting.\n",
    "\n",
    "Example in Random Forest:\n",
    "\n",
    "You have a dataset of 1,000 rows.\n",
    "\n",
    "For each tree:\n",
    "\n",
    "Randomly sample 1,000 rows with replacement → this is the bootstrap sample.\n",
    "\n",
    "Train the tree on this sample.\n",
    "\n",
    "Combine predictions from all trees via majority vote (classification) or averaging (regression).\n",
    "\n",
    "Quick Analogy:\n",
    "Imagine you’re training a group of students for a quiz. Instead of giving all of them the same set of practice questions, you give each one a random mix (some repeated, some missing).\n",
    "When you combine their answers, the group is likely to perform better overall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cfe62d-d14e-4025-8b0e-2d3675fc5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
    "evaluate ensemble models?\n",
    "\n",
    "Out-of-Bag (OOB) samples are the data points not selected in a particular bootstrap sample during the training of an ensemble model like Random Forest.\n",
    "\n",
    "How they are formed\n",
    "In bootstrap sampling, we randomly sample with replacement from the training set to create a dataset for each base learner.\n",
    "\n",
    "On average, each bootstrap sample contains about 63.2% of the original data.\n",
    "\n",
    "The remaining ~36.8% of the data is not used to train that specific model — these are the OOB samples for that model.\n",
    "\n",
    "Role of OOB samples\n",
    "Since OOB samples are not used for training a given model, they act like a built-in validation set for that model.\n",
    "\n",
    "For example:\n",
    "\n",
    "Train Tree 1 on Bootstrap Sample 1 → Evaluate Tree 1’s performance using its OOB samples.\n",
    "\n",
    "Train Tree 2 on Bootstrap Sample 2 → Evaluate using its OOB samples.\n",
    "…and so on.\n",
    "\n",
    "OOB Score\n",
    "The OOB score is an aggregate performance measure computed by:\n",
    "\n",
    "For each data point:\n",
    "\n",
    "Collect predictions from all models for which that point was an OOB sample.\n",
    "\n",
    "Compare these aggregated predictions with the true labels.\n",
    "\n",
    "Calculate an accuracy (classification) or R²/mean squared error (regression) metric.\n",
    "\n",
    "Advantages of OOB Score\n",
    "No need for a separate validation set — saves data.\n",
    "\n",
    "Built-in, unbiased performance estimate during training.\n",
    "\n",
    "Efficient — evaluation happens in parallel with model training.\n",
    "\n",
    "Analogy:\n",
    "Think of a classroom where each student (model) studies from a different set of practice problems (bootstrap sample).\n",
    "The questions they didn’t practice on (OOB samples) become a fair way to test them, and averaging all their test results gives the teacher (you) a reliable measure of overall skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a172e-c545-40fa-ad77-d291647afa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
    "Random Forest\n",
    "\n",
    "Here’s a clear comparison of how feature importance is determined in a single Decision Tree vs. a Random Forest:\n",
    "\n",
    "1. In a Single Decision Tree\n",
    "Calculation method:\n",
    "Feature importance is based on how much each feature reduces impurity across all splits where it’s used.\n",
    "\n",
    "For classification → uses Gini Impurity or Entropy.\n",
    "\n",
    "For regression → uses Variance Reduction.\n",
    "\n",
    "Process:\n",
    "\n",
    "At each split, calculate impurity reduction:\n",
    "\n",
    "Importance\n",
    "=\n",
    "Impurity(before split)\n",
    "−\n",
    "Weighted impurity(after split)\n",
    "Importance=Impurity(before split)−Weighted impurity(after split)\n",
    "Sum these reductions for each feature across the whole tree.\n",
    "\n",
    "Normalize so all feature importances sum to 1.\n",
    "\n",
    "Limitation:\n",
    "Can be unstable — a small change in data can change the top splits drastically.\n",
    "\n",
    "2. In a Random Forest\n",
    "Calculation method:\n",
    "Since Random Forest has many trees, importance is calculated by:\n",
    "\n",
    "Computing feature importance in each tree (as above).\n",
    "\n",
    "Averaging the importances over all trees.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "More stable: Averaging across many trees reduces sensitivity to noise.\n",
    "\n",
    "Better generalization: Random subsets of features at each split ensure that importance scores reflect multiple perspectives.\n",
    "\n",
    "Alternative method:\n",
    "Random Forests can also use permutation importance — shuffling a feature’s values and seeing how much the model performance drops.\n",
    "\n",
    "| Aspect               | Single Decision Tree                               | Random Forest                                   |\n",
    "| -------------------- | -------------------------------------------------- | ----------------------------------------------- |\n",
    "| **Stability**        | High variance (can change with small data changes) | Stable (averaged over many trees)               |\n",
    "| **Bias**             | Can favor features with more categories            | Averaging reduces this bias                     |\n",
    "| **Robustness**       | Sensitive to outliers/noise                        | More robust due to ensemble effect              |\n",
    "| **Interpretability** | Easier to visualize                                | Harder to interpret directly, but more reliable |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c15909-5a56-4cd3-bfe5-35232a7bc816",
   "metadata": {},
   "outputs": [],
   "source": [
    "Analogy:\n",
    "\n",
    "Single Decision Tree → One teacher grading students — their opinion may be biased.\n",
    "\n",
    "Random Forest → Many teachers grading independently, then averaging — more fair and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6d7df-e9fc-4a6e-9f6b-b78d4e871804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 6: Write a Python program to:\n",
    "● Load the Breast Cancer dataset using\n",
    "sklearn.datasets.load_breast_cancer()\n",
    "● Train a Random Forest Classifier\n",
    "● Print the top 5 most important features based on feature importance scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f049b7d-de56-4698-9f2e-89486b4b7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# 2. Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. Get feature importances\n",
    "importances = model.feature_importances_\n",
    "feature_names = np.array(data.feature_names)\n",
    "\n",
    "# 4. Sort and get top 5 features\n",
    "indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
    "top5_indices = indices[:5]\n",
    "\n",
    "# 5. Print top 5 features\n",
    "print(\"Top 5 Important Features:\")\n",
    "for idx in top5_indices:\n",
    "    print(f\"{feature_names[idx]}: {importances[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604e6ee-258f-47ac-a0c8-ae5c10f0e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "How it works\n",
    "Load data → load_breast_cancer() gives us features & target.\n",
    "\n",
    "Train model → RandomForestClassifier fits on the data.\n",
    "\n",
    "Get importance scores → model.feature_importances_ returns an array of importance values.\n",
    "\n",
    "Sort and display → We use argsort() to get the top 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9b2df-f1df-4584-b424-344b582c4f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7: Write a Python program to:\n",
    "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "● Evaluate its accuracy and compare with a single Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37169a-36a4-462b-b904-854685eac220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train a single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# 4. Train a Bagging Classifier with Decision Trees\n",
    "bagging = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging.predict(X_test)\n",
    "acc_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# 5. Print results\n",
    "print(\"Accuracy of Single Decision Tree:\", acc_dt)\n",
    "print(\"Accuracy of Bagging Classifier:\", acc_bagging)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd6595-634b-442f-a657-ca0ab91f9b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "How this works\n",
    "Load & split data → Uses train_test_split() for fair comparison.\n",
    "\n",
    "Single Decision Tree → Trains and tests on the dataset.\n",
    "\n",
    "Bagging Classifier → Creates multiple Decision Trees trained on bootstrap samples.\n",
    "\n",
    "Accuracy comparison → Usually, bagging performs slightly better due to reduced variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc060e41-123a-4fbf-a5c1-67cb53926893",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8: Write a Python program to:\n",
    "● Train a Random Forest Classifier\n",
    "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
    "● Print the best parameters and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7505b-fd2e-43b6-b7e4-61699162b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Create the Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 4. Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],  # Number of trees\n",
    "    'max_depth': [None, 5, 10, 15]        # Depth of trees\n",
    "}\n",
    "\n",
    "# 5. Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# 6. Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Get best parameters\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# 8. Predict using the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 9. Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Final Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d584b-d610-4410-9967-cecea1f3b1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "How this works:\n",
    "\n",
    "Loads Iris dataset.\n",
    "\n",
    "Splits data into 80% training and 20% testing.\n",
    "\n",
    "Uses RandomForestClassifier.\n",
    "\n",
    "Tunes n_estimators (number of trees) and max_depth (tree depth) via GridSearchCV with 5-fold cross-validation.\n",
    "\n",
    "Prints best parameters and final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f72c5-ae4f-4124-be5c-1eaedbe7ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9: Write a Python program to:\n",
    "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
    "Housing dataset\n",
    "● Compare their Mean Squared Errors (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2ddd1-751f-443d-8bc6-29d90df8b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1. Load California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Bagging Regressor with Decision Tree as base estimator\n",
    "bagging_reg = BaggingRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "# 4. Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predictions\n",
    "y_pred_bagging = bagging_reg.predict(X_test)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "\n",
    "# 6. Calculate Mean Squared Errors\n",
    "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "\n",
    "# 7. Print Results\n",
    "print(\"Bagging Regressor MSE:\", mse_bagging)\n",
    "print(\"Random Forest Regressor MSE:\", mse_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f09600-0d24-4dad-82a5-6016c6b1aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explanation:\n",
    "\n",
    "Bagging Regressor: Uses multiple Decision Trees trained on random subsets of the data and averages their predictions.\n",
    "\n",
    "Random Forest Regressor: Similar to bagging but adds extra randomness in feature selection, often improving performance.\n",
    "\n",
    "MSE (Mean Squared Error) is used to compare prediction quality (lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadfaae-e18f-4a34-b0fc-7a1f204b5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
    "default. You have access to customer demographic and transaction history data.\n",
    "You decide to use ensemble techniques to increase model performance.\n",
    "Explain your step-by-step approach to:\n",
    "● Choose between Bagging or Boosting\n",
    "● Handle overfitting\n",
    "● Select base models\n",
    "● Evaluate performance using cross-validation\n",
    "● Justify how ensemble learning improves decision-making in this real-world\n",
    "context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e1273-2167-4a67-afe9-152cbd3ebca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
